{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae06f17",
   "metadata": {},
   "source": [
    "##### Step 1: Make your func to extract data from the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e5348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz #PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf):\n",
    "            page_text = page.get_text()\n",
    "            text += page_text + \"\\n\"\n",
    "            print(f\"✅ Extracted Page {page_num + 1}\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9a3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted Page 1\n",
      "✅ Extracted Page 2\n",
      "✅ Extracted Page 3\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"media\\DBMS_Syllabus.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25ee14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"syllabus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaeb000",
   "metadata": {},
   "source": [
    "##### Step 2: Chunking the extracted-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1370c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=500):\n",
    "    sentences = text.split('. ')\n",
    "    chunks, chunk = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < max_length:\n",
    "            chunk += sentence + '. '\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + '. '\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(extracted_text, max_length=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fe6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks created: 8\n",
      "This is a sample chunk: Review the fundamental view on unstructured data and describe other emerging\n",
      "database technologies.\n",
      "Module:1 Database \n",
      "Systems \n",
      "Concepts \n",
      "and \n",
      "Architecture \n",
      "4 hours \n",
      "Need  for  database  systems  – Characteristics  of  Database Approach – Advantages of \n",
      "using DBMS approach -  Actors on the Database Management Scene: Database \n",
      "Administrator - Classification  of database management systems  -  Data Models -  Schemas \n",
      "and Instances - Three-Schema Architecture  -   The  Database  System  Environment - \n",
      "Centralized  and  Client/Server  Architectures  for  DBMSs – Overall Architecture of \n",
      "Database Management Systems  \n",
      "Module:2  Relational Model and E-R Modeling \n",
      "6 hours \n",
      "Relational Model:  Candidate Keys, Primary Keys, Foreign Keys -  Integrity Constraints - \n",
      "Handling of Nulls - Entity  Relationship  Model: Types  of  Attributes, Relationships, \n",
      "Structural Constraints, Relational model Constraints – Mapping ER model to a relational \n",
      "schema – Extended ER Model - Generalization – Specialization – Aggregations.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Chunks created: {len(chunks)}\")\n",
    "print(f\"This is a sample chunk: {chunks[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d39860",
   "metadata": {},
   "source": [
    "##### Step 3: Creating Embeddings for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b003e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\PROJECTS\\BasicSyllabusRAG\\env_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab68716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d07c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: \n",
      " (8, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Embeddings Shape: \\n {np.array(embeddings).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7202e49",
   "metadata": {},
   "source": [
    "##### Step 4: Adding embeddings into the FAISS for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d54282",
   "metadata": {},
   "source": [
    "Seeing if it has been already been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebceafac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index loaded.\n",
      "✅ Loaded 8 chunks.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"syllabus_index.faiss\")\n",
    "print(\"✅ FAISS index loaded.\")\n",
    "\n",
    "# Load chunks list\n",
    "with open(\"syllabus_chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d85f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and 8 items added.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# FAISS requires data in numpy float32 format to build its index.\n",
    "embeddings_np = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# IndexFlatL2 creates a flat (brute-force) index using L2 (Euclidean) distance for similarity search.\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"Index created and {index.ntotal} items added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ac413",
   "metadata": {},
   "source": [
    "##### Step 5: Saving the embeddings for faster loading time w.r.t same pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c29b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index saved to syllabus_index.faiss\n",
      "✅ Chunks saved to syllabus_chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "faiss.write_index(index, \"syllabus_index.faiss\")\n",
    "print(\"✅ FAISS index saved to syllabus_index.faiss\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"syllabus_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(\"✅ Chunks saved to syllabus_chunks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07fc9135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Dimensions: 384\n",
      "Number of vectors stored: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Index Dimensions: {index.d}\")\n",
    "print(f\"Number of vectors stored: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdfd02",
   "metadata": {},
   "source": [
    "##### Step 6: Time to retrieve the relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df3cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top - k embeddings dependent on the query.\n",
    "def retrieve(query, k = 3):\n",
    "    #Encode the query into an embedding\n",
    "    query_embedding = model.encode([query]).astype(\"float32\")\n",
    "    \n",
    "    #Search the FAISS index for k-similar chunks\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    result = [chunks[i] for i in indices[0]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "216ca605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 results for your query.\n",
      "\n",
      "Result 1: \n",
      "Gerardus Blokdyk, NoSQL Databases A Complete Guide, 5STARCooks, 2021 \n",
      "Mode of Evaluation: CAT, Written assignments, Quiz and FAT. \n",
      "Recommended by Board of Studies \n",
      "04-03-2022 \n",
      "Approved by Academic Council \n",
      "No. 65 \n",
      "Date \n",
      "17-03-2022 \n",
      " \n",
      " \n",
      " \n",
      "Agenda Item 65/39 - Annexure - 35\n",
      "Proceedings of the 65th Academic Council (17.03.2022)\n",
      "985\n",
      "\n",
      ".\n",
      "--------------------------------------------------\n",
      "Result 2: \n",
      "BCSE302L \n",
      "Database Systems (3-0-0-3) \n",
      "Introduction to Data Models - Various architecture of DBMS - Different Relational Models - \n",
      "Entity and relations model - Different types of Normalization – Types of indexing - Hashing \n",
      "Techniques -  Query processing - Query optimization techniques - Transaction processing -  \n",
      "Concurrency control - Introduction to NoSQL databases.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Time to test it with a sample query\n",
    "\n",
    "query = \"What is NoSQL\"\n",
    "k = 2\n",
    "\n",
    "results = retrieve(query, k)\n",
    "\n",
    "print(f\"Top {len(results)} results for your query.\\n\")\n",
    "for i , res in enumerate(results, 1):\n",
    "    print(f\"Result {i}: \\n{res}\\n{\"-\"*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafccada",
   "metadata": {},
   "source": [
    "##### Step 7: Connecting with the Ollama Local Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad6b70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "def generate_answer_ollama(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an academic assistant.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    You have been provided with the following context, use this to generate a better answer.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat(\n",
    "        model='llama2:7b',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f16567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The CAP Theorem, also known as the Consistency, Availability, and Partition Tolerance theorem, is a fundamental concept in NoSQL databases. It was first introduced by Jim Gray and Michael Stonebraker in 1978 and has since become a cornerstone of modern distributed systems.\n",
      "\n",
      "The CAP Theorem states that in a distributed database system, it is impossible to simultaneously achieve all three of the following properties:\n",
      "\n",
      "1. Consistency: Every client request receives a consistent view of the data.\n",
      "2. Availability: The system is always available to process requests, even in the presence of failures.\n",
      "3. Partition Tolerance: The system continues to function and serve requests even when there are network partitions (i.e., some nodes in the system cannot communicate with each other).\n",
      "\n",
      "In other words, the CAP Theorem dictates that a distributed database system can only choose two out of these three properties to prioritize. For example, a system might prioritize consistency over availability, or availability over partition tolerance. The choice depends on the specific use case and requirements of the application.\n",
      "\n",
      "The CAP Theorem has important implications for the design and implementation of NoSQL databases. It highlights the trade-offs that are inherent in distributed systems and emphasizes the need for careful consideration when making design decisions. By understanding the limitations of the CAP Theorem, developers can create more robust and reliable distributed systems that meet the needs of their applications.\n",
      "\n",
      "References:\n",
      "\n",
      "* Gray, J., & Stonebraker, M. (1978). The CAP theorem: A collection of independent results. ACM Transactions on Database Systems (TODS), 3(2), 1-50.\n",
      "* NoSQL Databases: A Complete Guide by Gerardus Blokdyk, 5STARCooks, 2021.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is CAP Theorem in NoSQL\"\n",
    "results = retrieve(query, k=1)\n",
    "answer = generate_answer_ollama(query, results)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
